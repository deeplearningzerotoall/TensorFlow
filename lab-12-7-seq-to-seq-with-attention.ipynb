{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12-7 sequence to sequence with attention\n",
    "### simple neural machine translation training \n",
    "* sequence to sequence\n",
    "* variable input sequence length\n",
    "* variable output sequence length\n",
    "* Luong attention\n",
    "  \n",
    "### Reference\n",
    "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from pprint import pprint\n",
    "\n",
    "s2s = tf.contrib.seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepairing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'changing': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'very': 14}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for sources\n",
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = ['<pad>'] + s_vocab\n",
    "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
    "\n",
    "pprint(source2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 1,\n",
      " '<eos>': 2,\n",
      " '<pad>': 0,\n",
      " '고프다': 3,\n",
      " '나는': 4,\n",
      " '딥러닝을': 5,\n",
      " '매우': 6,\n",
      " '배가': 7,\n",
      " '변화한다': 8,\n",
      " '빠르게': 9,\n",
      " '어렵다': 10,\n",
      " '위한': 11,\n",
      " '텐서플로우는': 12,\n",
      " '프레임워크이다': 13}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for targets\n",
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
    "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
    "\n",
    "pprint(target2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
    "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source':\n",
    "        # preprocessing for source (encoder)\n",
    "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
    "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target':\n",
    "        # preprocessing for target (decoder)\n",
    "        # input\n",
    "        t_input = list(map(lambda sentence : ['<bos>'] + sentence, sequences))\n",
    "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
    "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
    "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        # output\n",
    "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
    "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
    "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        return t_len, t_input, t_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5] [[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing for source\n",
    "s_max_len = 10\n",
    "s_len, s_input = preprocess(sequences = sources,\n",
    "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
    "print(s_len, s_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 5, 5] [[ 1  4  7  3  0  0  0  0  0  0  0  0]\n",
      " [ 1 12  6 10  0  0  0  0  0  0  0  0]\n",
      " [ 1 12  5 11 13  0  0  0  0  0  0  0]\n",
      " [ 1 12  6  9  8  0  0  0  0  0  0  0]] [[ 4  7  3  2  0  0  0  0  0  0  0  0]\n",
      " [12  6 10  2  0  0  0  0  0  0  0  0]\n",
      " [12  5 11 13  2  0  0  0  0  0  0  0]\n",
      " [12  6  9  8  2  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing for target\n",
    "t_max_len = 12\n",
    "t_len, t_input, t_output = preprocess(sequences = targets,\n",
    "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
    "print(t_len, t_input, t_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 100\n",
    "batch_size = 2\n",
    "lr = .3\n",
    "\n",
    "# input\n",
    "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
    "data = data.shuffle(buffer_size = 10)\n",
    "data = data.batch(batch_size = batch_size)\n",
    "iterator = data.make_initializable_iterator()\n",
    "s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encoder\n",
    "# hyper-parameters for encoder, decoder (sequence to sequence), one-hot encoding\n",
    "n_of_classes = len(target2idx)\n",
    "enc_hidden_dim = 10\n",
    "dec_hidden_dim = 5\n",
    "\n",
    "# one-hot encoding\n",
    "s_embedding = tf.eye(num_rows = len(source2idx))\n",
    "s_embedding = tf.get_variable(name = 'source_embedding', initializer = s_embedding, trainable = False)\n",
    "\n",
    "# embedding layer\n",
    "s_mb_batch = tf.nn.embedding_lookup(params = s_embedding, ids = s_mb_input)\n",
    "\n",
    "# encoder (lstm_cell)\n",
    "enc_cell = tf.nn.rnn_cell.LSTMCell(num_units = enc_hidden_dim, dtype = tf.float32)\n",
    "enc_outputs, _ = tf.nn.dynamic_rnn(cell = enc_cell, inputs = s_mb_batch, sequence_length = s_mb_len,\n",
    "                                 dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decoder\n",
    "t_embedding = tf.eye(num_rows = len(target2idx))\n",
    "t_embedding = tf.get_variable(name = 'target_embedding', initializer = t_embedding, trainable = False)\n",
    "\n",
    "# embedding layer\n",
    "t_mb_batch = tf.nn.embedding_lookup(params = t_embedding, ids = t_mb_input)\n",
    "\n",
    "batch_size = tf.reduce_sum(tf.ones_like(tensor = s_mb_len, dtype = tf.int32))\n",
    "tr_tokens = tf.tile(input = [t_max_len], multiples = [batch_size])\n",
    "trans_tokens = tf.tile(input = [target2idx.get('<bos>')], multiples = [batch_size])\n",
    "\n",
    "# decoder (lstm_cell) with Luong attention\n",
    "dec_cell = tf.nn.rnn_cell.LSTMCell(num_units = dec_hidden_dim, dtype = tf.float32)\n",
    "luong_attn = s2s.LuongAttention(num_units = dec_hidden_dim, memory = enc_outputs, \n",
    "                                memory_sequence_length = s_mb_len)\n",
    "dec_attn_cell = s2s.AttentionWrapper(cell = dec_cell, attention_mechanism = luong_attn)\n",
    "dec_attn_init_state = dec_attn_cell.zero_state(batch_size = batch_size, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추후에 keras.layers.Dense로 교체될 듯, 아직 tf.contrib.seq2seq package가 tf.keras.layers를 지원하지 않음\n",
    "output_layer = tf.layers.Dense(units = n_of_classes) \n",
    "\n",
    "# decoder for training\n",
    "tr_helper = s2s.TrainingHelper(inputs = t_mb_batch, sequence_length = tr_tokens)\n",
    "tr_decoder = s2s.BasicDecoder(cell = dec_attn_cell, helper = tr_helper, initial_state = dec_attn_init_state,\n",
    "                              output_layer = output_layer)\n",
    "tr_outputs, _, _ = s2s.dynamic_decode(decoder = tr_decoder, impute_finished = True,\n",
    "                                      maximum_iterations = t_max_len)\n",
    "\n",
    "# decoder for translation\n",
    "trans_helper = s2s.GreedyEmbeddingHelper(embedding = t_embedding,\n",
    "                                         start_tokens = trans_tokens, end_token = target2idx.get('<eos>'))\n",
    "trans_decoder = s2s.BasicDecoder(cell = dec_attn_cell, helper = trans_helper, initial_state = dec_attn_init_state,\n",
    "                                 output_layer = output_layer)\n",
    "trans_outputs, _, _ = s2s.dynamic_decode(decoder = trans_decoder, impute_finished = True,\n",
    "                                         maximum_iterations = t_max_len * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss\n",
    "masking = tf.sequence_mask(lengths = t_mb_len,\n",
    "                           maxlen = t_max_len, dtype = tf.float32)\n",
    "loss = s2s.sequence_loss(logits = tr_outputs.rnn_output, \n",
    "                         targets = t_mb_output,\n",
    "                         weights = masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training\n",
    "opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "training_op = opt.minimize(loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config = sess_config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  10, tr_loss : 1.122\n",
      "epoch :  20, tr_loss : 0.416\n",
      "epoch :  30, tr_loss : 0.317\n",
      "epoch :  40, tr_loss : 0.265\n",
      "epoch :  50, tr_loss : 0.322\n",
      "epoch :  60, tr_loss : 0.210\n",
      "epoch :  70, tr_loss : 0.114\n",
      "epoch :  80, tr_loss : 0.117\n",
      "epoch :  90, tr_loss : 0.082\n",
      "epoch : 100, tr_loss : 0.006\n"
     ]
    }
   ],
   "source": [
    "tr_loss_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sess.run(iterator.initializer)\n",
    "    avg_tr_loss = 0\n",
    "    tr_step = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            _, tr_loss = sess.run(fetches = [training_op, loss])\n",
    "            avg_tr_loss += tr_loss\n",
    "            tr_step += 1\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    \n",
    "    avg_tr_loss /= tr_step\n",
    "    tr_loss_hist.append(avg_tr_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('epoch : {:3}, tr_loss : {:.3f}'.format(epoch + 1, avg_tr_loss))             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_output_hat = sess.run(trans_outputs.sample_id,\n",
    "                        feed_dict = {s_mb_len : s_len,\n",
    "                                     s_mb_input : s_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['나는', '배가', '고프다', '<eos>', '<pad>'],\n",
       " ['텐서플로우는', '매우', '어렵다', '<eos>', '<pad>'],\n",
       " ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다', '<eos>'],\n",
       " ['텐서플로우는', '매우', '빠르게', '변화한다', '<eos>']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda sentence : [idx2target.get(token) for token in sentence], t_output_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
