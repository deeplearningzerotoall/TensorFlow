{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lab-02 Simple Liner Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf.enable_eager_execution() 은 그래프가 아닌 바로 계산이 필요한 상황일 때 사용한다고 한다.\n",
    "##### 이 함수를 쓰지 않으면 GradientTape 를 쓰기 어렵다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3, 4, 5]\n",
    "y_data = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis = W * X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 가설함수를 세운다. 어떤 상황에 대한 일률적인 틀?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = W * x_data + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 가설과 실제값의 차이가 있을 거다.\n",
    "##### 그 차이를 우리는 error라고 한다. // error = hypothesis - y_data\n",
    "##### 에러가 + - 인 상황이 있다 보니 다 제곱해서 양의 숫자로 변환한다.\n",
    "##### 그리고 그 값들의 평균을 낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate initialize\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### cost 값이 너무 크면 안 된다.\n",
    "###### 예측값이랑 실제값이 차이가 크다는 거니까\n",
    "###### 그래서 그 차이를 최소화 하려고 한다.\n",
    "\n",
    "###### 어떻게?\n",
    "###### 미분시켜버"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    hypothesis = W * x_data + b\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "W_grad, b_grad = tape.gradient(cost, [W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameter Update / 여기서 안 되면 보통 tf.eager_execution() 을 안 넣었을 때임\n",
    "###### 우일's 답변: 이건 TF 1.0임, 2.0으로 넘어가면 eager_execution() 안 써도 됨\n",
    "###### .\n",
    "###### 그래서 Cost가 크니까 다시 W랑 b를 조정하면서 Cost 값을 줄여봐야 한다.\n",
    "###### 어떻게 줄이지?\n",
    "###### Gradient Tape 을 쓰는 이유가 그것\n",
    "###### 이론은 다음 장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|    2.4520|    0.3760| 45.660004\n",
      "   10|    1.1036|    0.0034|  0.206336\n",
      "   20|    1.0128|   -0.0209|  0.001026\n",
      "   30|    1.0065|   -0.0218|  0.000093\n",
      "   40|    1.0059|   -0.0212|  0.000083\n",
      "   50|    1.0057|   -0.0205|  0.000077\n",
      "   60|    1.0055|   -0.0198|  0.000072\n",
      "   70|    1.0053|   -0.0192|  0.000067\n",
      "   80|    1.0051|   -0.0185|  0.000063\n",
      "   90|    1.0050|   -0.0179|  0.000059\n",
      "  100|    1.0048|   -0.0173|  0.000055\n"
     ]
    }
   ],
   "source": [
    "for i in range(100+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    #assign: 값을 완전히 할당. =에 해당\n",
    "    #assign_add: 값을 증가. +=에 해당\n",
    "    #assign_sub: 값을 감소. -=에 해당\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "      print(\"{:5}|{:10.4f}|{:10.4f}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.00667, shape=(), dtype=float32)\n",
      "tf.Tensor(2.4946702, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(W * 5 + b)\n",
    "print(W * 2.5 + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 우리가 W = 1 을 b = 0 을 원했을 때 적합한 값이 나옴"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
